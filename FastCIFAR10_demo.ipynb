{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast CIFAR10 Training\n",
    "\n",
    "Based on work by David C. Page: https://github.com/davidcpage/cifar10-fast\n",
    "\n",
    "This implementation was built on work by Thomas Germer: https://github.com/99991/cifar10-fast-simple/\n",
    "\n",
    "Also incorporates ideas from Fern: https://github.com/tysam-code/hlb-CIFAR10 and Jordan Keller: https://github.com/KellerJordan/cifar10-airbench/\n",
    "\n",
    "The main focus of this implementation is to be readable for deep learning beginners, and use built-in pytorch functions as much as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn\n",
    "\n",
    "# Torchvision imports\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# For timing training/testing loops\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# For visualisation of LR schedule curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Precision for training speedup\n",
    "dtype = torch.float16 if device.type != \"cpu\" else torch.float32\n",
    "\n",
    "# Show package versions and hardware\n",
    "print(f\"PyTorch version: {torch.__version__}\\nTorchvision version: {torchvision.__version__}\\n\")\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(device.index)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, dtype=torch.float16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Change dataset datatype to dtype, then send to GPU. \n",
    "    Calculate mean and std from dataset, then normalize. \n",
    "    Permute data dimensions from NHWC to NCWH. \n",
    "    Pad Training Images.\n",
    "    \"\"\"\n",
    "    # Cast dataset data to tensor of dtype, place on GPU\n",
    "    data = torch.from_numpy(dataset.data).to(dtype).to(device)\n",
    "\n",
    "    # Normalize data\n",
    "    # type and to might be redundant here\n",
    "    mean = torch.mean(data, axis=(0,1,2))\n",
    "    std = torch.std(data, axis=(0,1,2))\n",
    "    data = (data - mean) / std\n",
    "\n",
    "    # permute data NHWC to NCWH\n",
    "    # See: https://discuss.pytorch.org/t/why-does-pytorch-prefer-using-nchw/83637/4\n",
    "    data = data.permute(0, 3, 1, 2)\n",
    "\n",
    "    # pad training images\n",
    "    # functional.pad adds padding to 2 last dims of tensor, so perform after permute\n",
    "    if dataset.train:\n",
    "        data = v2.functional.pad(data, 4, padding_mode=\"reflect\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_data(data_path=\"data\", dtype=torch.float16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load CIFAR10 dataset. \n",
    "    Preprocess train and Validation sets.\n",
    "    \"\"\"\n",
    "\n",
    "    train = datasets.CIFAR10(root=data_path, train=True, download=True)\n",
    "    test = datasets.CIFAR10(root=data_path, train=False, download=True)\n",
    "\n",
    "    X_train = preprocess_data(train, dtype, device)\n",
    "    X_test = preprocess_data(test, dtype, device)\n",
    "\n",
    "    y_train = torch.tensor(train.targets).to(device)\n",
    "    y_test = torch.tensor(test.targets).to(device)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patch-based PCA Whitening weights initialisation\n",
    "# Implementation by Thomas Germer: https://github.com/99991/cifar10-fast-simple/\n",
    "# Refer to Keller Jordan's paper for explanation:  https://arxiv.org/abs/2404.00498\n",
    "\n",
    "def patch_whitening(data, patch_size=(3, 3)):\n",
    "    # Compute weights from data such that\n",
    "    # torch.std(F.conv2d(data, weights), dim=(2, 3))is close to 1.\n",
    "    h, w = patch_size\n",
    "    c = data.size(1)\n",
    "    patches = data.unfold(2, h, 1).unfold(3, w, 1)\n",
    "    patches = patches.transpose(1, 3).reshape(-1, c, h, w).to(torch.float32)\n",
    "\n",
    "    n, c, h, w = patches.shape\n",
    "    X = patches.reshape(n, c * h * w)\n",
    "    X = X / (X.size(0) - 1) ** 0.5\n",
    "    covariance = X.t() @ X\n",
    "\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(covariance)\n",
    "\n",
    "    eigenvalues = eigenvalues.flip(0)\n",
    "\n",
    "    eigenvectors = eigenvectors.t().reshape(c * h * w, c, h, w).flip(0)\n",
    "\n",
    "    return eigenvectors / torch.sqrt(eigenvalues + 1e-2).view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ghost BatchNorm \n",
    "# Implementation by Thomas Germer: https://github.com/99991/cifar10-fast-simple/\n",
    "# Designed by Hoffer, Hubara, Soudry: https://arxiv.org/abs/1705.08741\n",
    "\n",
    "class GhostBatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, num_splits, **kw):\n",
    "        super().__init__(num_features, **kw)\n",
    "\n",
    "        running_mean = torch.zeros(num_features * num_splits)\n",
    "        running_var = torch.ones(num_features * num_splits)\n",
    "\n",
    "        self.weight.requires_grad = False\n",
    "        self.num_splits = num_splits\n",
    "        self.register_buffer(\"running_mean\", running_mean)\n",
    "        self.register_buffer(\"running_var\", running_var)\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        if (self.training is True) and (mode is False):\n",
    "            # lazily collate stats when we are going to use them\n",
    "            self.running_mean = torch.mean(\n",
    "                self.running_mean.view(self.num_splits, self.num_features), dim=0\n",
    "            ).repeat(self.num_splits)\n",
    "            self.running_var = torch.mean(\n",
    "                self.running_var.view(self.num_splits, self.num_features), dim=0\n",
    "            ).repeat(self.num_splits)\n",
    "        return super().train(mode)\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.shape\n",
    "        if self.training or not self.track_running_stats:\n",
    "            assert n % self.num_splits == 0, f\"Batch size ({n}) must be divisible by num_splits ({self.num_splits}) of GhostBatchNorm\"\n",
    "            return F.batch_norm(\n",
    "                input.view(-1, c * self.num_splits, h, w),\n",
    "                self.running_mean,\n",
    "                self.running_var,\n",
    "                self.weight.repeat(self.num_splits),\n",
    "                self.bias.repeat(self.num_splits),\n",
    "                True,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            ).view(n, c, h, w)\n",
    "        else:\n",
    "            return F.batch_norm(\n",
    "                input,\n",
    "                self.running_mean[: self.num_features],\n",
    "                self.running_var[: self.num_features],\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                False,\n",
    "                self.momentum,\n",
    "                self.eps,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fast CIFAR10 Resnet \"Bag of Tricks\" architecture by David C. Page\n",
    "# http://web.archive.org/web/20201123223831/https://myrtle.ai/learn/how-to-train-your-resnet-8-bag-of-tricks/\n",
    "\n",
    "class FastCIFAR_BoT(nn.Module):\n",
    "    def __init__(self, first_layer_weights: torch.Tensor, celu_alpha: float, \n",
    "                 input_shape: int, hidden_units: int, output_shape: int, output_scale: float):\n",
    "        super().__init__()\n",
    "\n",
    "        ## First layer contains patch whitened initialised weights and is frozen.\n",
    "        conv1_out_shape = first_layer_weights.size(0)\n",
    "        conv1 = nn.Conv2d(in_channels=input_shape, out_channels=conv1_out_shape, \n",
    "                          kernel_size=3, padding=1, bias=False)\n",
    "        conv1.weight.data = first_layer_weights\n",
    "        conv1.weight.requires_grad = False\n",
    "\n",
    "        self.conv1 = conv1\n",
    "\n",
    "        # From here, modified original arch:\n",
    "        # Added GhostBatchNorm, add CELU, Move max pool2d to second layer.\n",
    "        # Prep layer\n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=conv1_out_shape, out_channels=hidden_units,\n",
    "                      kernel_size=1, padding=0, bias=False),\n",
    "            GhostBatchNorm(hidden_units, num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "        # Layer 1 with residual connections and 2 res sequences\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=(hidden_units*2),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            GhostBatchNorm((hidden_units*2), num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "        self.res1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*2), out_channels=(hidden_units*2),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            GhostBatchNorm((hidden_units*2), num_splits=16),\n",
    "            nn.CELU(alpha=0.3),\n",
    "        )\n",
    "        self.res2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*2), out_channels=(hidden_units*2),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            GhostBatchNorm((hidden_units*2), num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*2), out_channels=(hidden_units*4),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            GhostBatchNorm(hidden_units*4, num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "        # Setup of Layer 3 etc is the same as layer 1 etc.\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*4), out_channels=(hidden_units*8),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            GhostBatchNorm(hidden_units*8, num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "        self.res3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*8), out_channels=(hidden_units*8),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            GhostBatchNorm(hidden_units*8, num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "        self.res4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=(hidden_units*8), out_channels=(hidden_units*8),\n",
    "                      kernel_size=3, padding=1, bias=False),\n",
    "            GhostBatchNorm(hidden_units*8, num_splits=16),\n",
    "            nn.CELU(alpha=celu_alpha),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=(hidden_units*8),out_features=output_shape, \n",
    "                      bias=False),\n",
    "        )\n",
    "\n",
    "        # Scale parameter for final layer\n",
    "        self.output_scale = output_scale\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.prep(x)\n",
    "        x = self.layer1(x)\n",
    "        x = x + self.res2(self.res1(x))         # Residual connection\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x + self.res4(self.res3(x))         # Residual connection\n",
    "        x = self.classifier(x)\n",
    "        x = torch.mul(x, self.output_scale)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise training and validation models\n",
    "\n",
    "# initialise weights of first layer with patch-whitening.\n",
    "first_layer_weights = patch_whitening(X_train[:, :, 4:-4, 4:-4])\n",
    "\n",
    "# Initialise training model\n",
    "model = FastCIFAR_BoT(first_layer_weights, celu_alpha=0.3, input_shape=3, hidden_units=64, output_shape=10, output_scale=0.125)    \n",
    "\n",
    "# Set model weights to half precision (torch.float16) for faster training\n",
    "model.to(dtype)\n",
    "\n",
    "# Set BatchNorm layers back to single precision (better accuracy)\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        module.float()\n",
    "\n",
    "# Upload model to GPU\n",
    "model.to(device)\n",
    "\n",
    "## Initialise validation model\n",
    "# Receives Exponential Moving Average weight update every 5 batches\n",
    "val_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters for Loss Function and Optimizer:\n",
    "# Training length parameters\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "# Loss function parameters:\n",
    "label_smoothing = 0.2\n",
    "\n",
    "# Optimizer parameters\n",
    "bias_scaler = 64\n",
    "lr_weights = 0.512 / batch_size                   # apply linear scaling rule\n",
    "lr_bias = 0.512 * bias_scaler/batch_size\n",
    "wd_weights = 0.0005 * batch_size\n",
    "wd_biases = 0.0005 * bias_scaler/batch_size\n",
    "momentum = 0.9\n",
    "nesterov = True\n",
    "\n",
    "# LambdaLR Scheduler helper parameters for Page-like learning curve\n",
    "linear_factor = 0.1\n",
    "phase1 = 0.2 \n",
    "phase2 = 0.8\n",
    "total_train_steps = epochs * int(50000/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise loss function\n",
    "# Use label smoothing for regularization\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=label_smoothing,reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for LambdaLR Learning Rate Scheduler\n",
    "def lr_lambda_helper(linear_factor,phase1,phase2,total_train_steps):\n",
    "    \"\"\"\n",
    "    Returns Lambda LR scheduling helper. \n",
    "    Scales defined LR in 3 phases:\n",
    "    Phase 1: Increasing LR for (p1*total_steps) steps, by factor 0 to 1.\n",
    "    Phase 2: Decreasing LR for (p2*total_steps) steps, by factor 1 to linear_factor.\n",
    "    Phase 3: Constant LR at scale linear_factor.\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = int(phase1*total_train_steps)\n",
    "    p2 = int(phase2*total_train_steps)\n",
    "\n",
    "    def helper(step):\n",
    "        if step < p1:                       # Phase 1\n",
    "            return step / p1    \n",
    "        elif step < (p1 + p2):              # Phase 2\n",
    "            return 1 - ((step - p1) * (1 - linear_factor) / p2)\n",
    "        else:\n",
    "            return linear_factor            # Phase 3: Constant LR\n",
    "        \n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Learning Rate Scaling curve: \n",
    "lr_demo_optim = SGD(torch.nn.Linear(10, 2).parameters())\n",
    "lr_lambda = lr_lambda_helper(linear_factor,phase1,phase2,total_train_steps)\n",
    "demo_sch = LambdaLR(lr_demo_optim, lr_lambda=lr_lambda)\n",
    "\n",
    "lrs = []\n",
    "for step in range(970):\n",
    "    lrs.append(demo_sch.get_last_lr()[0])\n",
    "    lr_demo_optim.step()     \n",
    "    demo_sch.step()\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.plot(range(970), lrs, label='Learning Rate')\n",
    "plt.title('Custom Learning Rate Schedule')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialise Optimizers and Learning Rate scheduler\n",
    "# For separate updating of weights and biases:\n",
    "weights = [par for name, par in model.named_parameters() if \"weight\" in name and par.requires_grad]\n",
    "biases = [par for name, par in model.named_parameters() if \"bias\" in name and par.requires_grad]\n",
    "\n",
    "# Initialise Stochastic Gradient Descent optimizers for weights and biases\n",
    "optim_weight = SGD(params=weights, lr=lr_weights, weight_decay=wd_weights, momentum=momentum, nesterov=nesterov)\n",
    "optim_bias = SGD(params=biases, lr=lr_bias, weight_decay=wd_biases, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "# Learning rate schedules\n",
    "lr_lambda = lr_lambda_helper(linear_factor,phase1,phase2,total_train_steps)\n",
    "lr_sched_w = LambdaLR(optim_weight, lr_lambda=lr_lambda)\n",
    "lr_sched_b = LambdaLR(optim_bias, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    ## Training Loop inits\n",
    "    batch_count = 0\n",
    "    total_train_time = 0\n",
    "    epochs=10\n",
    "\n",
    "    # Print stats:\n",
    "    print(\"epoch    batch    train time [sec]    validation accuracy\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Wait for all cuda streams on device to complete\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Start epoch timer\n",
    "        time_start_epoch = timer()\n",
    "\n",
    "        ## Regularization transforms:\n",
    "        # Alternating Flip (Jordan Keller); Can possibly speed up by avoiding torch.cat\n",
    "        if epoch % 2:\n",
    "            X_data = v2.functional.horizontal_flip(X_train[:25000])\n",
    "            X_data = torch.cat((X_data,X_train[25000:]))\n",
    "        else:\n",
    "            X_data = v2.functional.horizontal_flip(X_train[25000:])\n",
    "            X_data = torch.cat((X_train[:25000],X_data))\n",
    "        \n",
    "        # Random crop training images to 32x32\n",
    "        X_data = v2.RandomCrop(size=(32,32))(X_data)\n",
    "\n",
    "        ## Randomly Erase 8x8 square from training image. Generally lowers validation accuracy.\n",
    "        #X_data = v2.RandomErasing(p=0.5, scale=(0.25, 0.25), ratio=(1,1))(X_data)\n",
    "\n",
    "        # Shuffle training data\n",
    "        idx = torch.randperm(len(X_data), device=device)\n",
    "        X_data = X_data[idx]\n",
    "        y_data = y_train[idx]\n",
    "\n",
    "        # Iterate over batches:\n",
    "        for i in range(0, len(X_data), batch_size):\n",
    "            # Discard partial batch (last batch is 336 images)\n",
    "            if i+batch_size > len(X_data):\n",
    "                break\n",
    "\n",
    "            # select batch slice from training data\n",
    "            X_batch = X_data[i:i+batch_size]\n",
    "            y_true = y_data[i:i+batch_size]\n",
    "\n",
    "            # Zero gradients before update, set model to train\n",
    "            model.zero_grad()\n",
    "            model.train(True)\n",
    "\n",
    "            # Forward Pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "            # Backward pass\n",
    "            # Summed loss, refer Page\n",
    "            loss.sum().backward()\n",
    "\n",
    "            # Optimizer and LR step\n",
    "            optim_weight.step()\n",
    "            optim_bias.step()\n",
    "\n",
    "            lr_sched_w.step()\n",
    "            lr_sched_b.step()\n",
    "\n",
    "            # update Validation model parameters with EMA every 5 batches\n",
    "            if (i // batch_size % 5) == 0:\n",
    "                val_model.update_parameters(model)\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "        # Add epoch time to total\n",
    "        total_train_time += timer()-time_start_epoch\n",
    "\n",
    "        # Validation loop code taken directly from Thomas Germer\n",
    "        val_correct = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            val_model.train(False)\n",
    "\n",
    "            # Test time augmentation; Follows Page, Germer\n",
    "            regular_inputs = X_test[i : i + batch_size]\n",
    "            flipped_inputs = torch.flip(regular_inputs, [-1])\n",
    "\n",
    "            # val model logits\n",
    "            logits1 = val_model(regular_inputs).detach()\n",
    "            logits2 = val_model(flipped_inputs).detach()\n",
    "\n",
    "            # Final logits are average of augmented logits\n",
    "            logits = torch.mean(torch.stack([logits1, logits2], dim=0), dim=0)\n",
    "\n",
    "            # Compute correct predictions\n",
    "            correct = logits.max(dim=1)[1] == y_test[i : i + batch_size]\n",
    "\n",
    "            val_correct.append(correct.detach().type(torch.float64))\n",
    "\n",
    "        # Report validation accuracy\n",
    "        val_acc = torch.mean(torch.cat(val_correct)).item()\n",
    "        print(f\"{epoch:5} {batch_count:8d} {total_train_time:19.2f} {val_acc:22.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training for 10 epochs\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
